{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceLLM():\n",
    "    def __init__(self, model_name=\"meta-llama/Llama-2-7b-chat-hf\", verbose=False):\n",
    "        self.model_name = model_name\n",
    "        self.verbose = verbose\n",
    "        self.pipeline = self.load_pipeline(\n",
    "            model_id=self.model_name, verbose=self.verbose\n",
    "        )\n",
    "        self.chat_template = \"\"\"\n",
    "            You are a writing a report on the impact of a past flood described in {keyword}, written in the past tense. \n",
    "\n",
    "            You are given the following information about the flooding. Please read the information, summarise it and then turn that into a report.\n",
    "\n",
    "            Please cite all statements using the sources included in the information e.g. \"There were X casualties [1]\" . \n",
    "\n",
    "            Do not include information not provided above. Make sure to include all the facts given \n",
    "\n",
    "            {information}\n",
    "\n",
    "            Report:\"\"\"\n",
    "\n",
    "    def __call__(self, keyword, information):\n",
    "        prompt = self.chat_template.format(keyword=keyword, information=information)\n",
    "        response = self.pipeline(prompt)\n",
    "        return response\n",
    "\n",
    "    def load_pipeline(self, model_id=\"meta-llama/Llama-2-7b-chat-hf\", verbose=False):\n",
    "        # You can generically use AutoTokenizer.from_pretrained(model_id) to load any tokenizer\n",
    "        # but I've found that it's slower than the specific tokenizer.from_pretrained(model_id)\n",
    "        # if you know what it should be.\n",
    "        tokenizer = transformers.LlamaTokenizer.from_pretrained(model_id)\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        pipeline = HuggingFacePipeline(pipeline=pipeline, verbose=True)\n",
    "        return pipeline\n",
    "\n",
    "\n",
    "\n",
    "llm_model = HuggingFaceLLM()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = \"\"\"\n",
    "[’40 caravans were evacuated from Ballater Caravan Park. [1, 2]\\n300 properties in Brechin, Angus, were given the opportunity to evacuate and spend the night in a local school amid fears of further flooding. [3]’,\n",
    " ‘1 person was reported missing after being swept away by flooding along the River Don near Monymusk in Aberdeenshire on the afternoon of Friday, 18 November. [1] [2]’,\n",
    " ‘According to [1], there is no information in the context.\\nAccording to [2], there is no information about the number of people injured in the context.\\nAccording to [3], there is no information about it in the context.\\nAccording to [4], there is no information in the context about the number of people who have been injured.\\nAccording to [5], there is no information in the context about the number of people who have been injured.\\nAccording to [6], there is no information in the context.\\nAccording to [8], there is no information in the context.’,\n",
    " ‘There is no information about the estimated cost in the context. [1] [2] [3] [4] [5] [6] [8]’,\n",
    " ‘The flooding was caused by heavy rain that fell across Scotland from 16 November 2022. [1] The UK’s Met Office said close to a month’s worth of rain fell across parts of Aberdeenshire and Angus in 48 hours to 18 November. As much as 140 mm of rain was recorded in Charr in Aberdeenshire. The village of Aboyne in Aberdeenshire recorded 71.4 mm of rain in 24 hours on 18 November. [2]’,\n",
    " ‘The flooding is severe. [1], [2], [3], [4], [5], [6]’]\n",
    "\"\"\"\n",
    "response = llm_model(keyword=\"Flooding, November 2022, Scotland\",information=information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a post to the flask app\n",
    "url = \"http://127.0.0.1:5000/llm_prompt\"\n",
    "\n",
    "# define the payload\n",
    "payload = {\n",
    "    'inputs': 'Any weather events in the next 24 hours you think we should be aware of? For example, do any cities look like they might be in risk of flooding?'\n",
    "}\n",
    "\n",
    "# make the request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
